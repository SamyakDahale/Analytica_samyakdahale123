{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-24T03:23:18.610809Z",
     "start_time": "2024-10-24T03:23:17.915904Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load your dataset (ensure it's UTF-8 encoded)\n",
    "books_df = pd.read_excel(\"E:\\MGM UDICT\\Analytica\\c.xlsx\")\n",
    "\n",
    "# Drop rows with NaN values in the 'Tokenized Text' column\n",
    "books_df = books_df.dropna(subset=['Tokenized Text'])\n",
    "\n",
    "# Step 1: Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Step 2: Fit and transform the preprocessed text\n",
    "tfidf_matrix = vectorizer.fit_transform(books_df['Tokenized Text'])\n",
    "\n",
    "# Step 3: Add the book titles to the DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "tfidf_df['Title'] = books_df['Title'].values\n",
    "tfidf_df.set_index('Title', inplace=True)\n",
    "\n",
    "def identify_book(prompt):\n",
    "    # Preprocess the prompt (this should include the same steps as for your texts)\n",
    "    # Example preprocessing might include normalization, removing stopwords, etc.\n",
    "    \n",
    "    prompt_vector = vectorizer.transform([prompt])  # Vectorize the prompt\n",
    "\n",
    "    # Calculate cosine similarity between the prompt vector and all book vectors\n",
    "    similarities = cosine_similarity(prompt_vector, tfidf_matrix)\n",
    "\n",
    "    # Find the index of the book with the highest similarity score\n",
    "    most_similar_index = similarities.argmax()\n",
    "\n",
    "    # Return the title of the most similar book and the similarity score\n",
    "    return books_df['Title'].values[most_similar_index], similarities[0][most_similar_index]\n",
    "\n",
    "# Example usage with a prompt in Devanagari script\n",
    "prompt = \"God\"  # Example prompt in Hindi\n",
    "book_title, similarity_score = identify_book(prompt)\n",
    "print(f\"Most Relevant Book: {book_title}, Similarity Score: {similarity_score:.4f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Relevant Book: truth_is_god, Similarity Score: 0.6063\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T03:22:25.154147Z",
     "start_time": "2024-10-24T03:22:25.114139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random # STEP 3 \n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def find_context_sentences(sentences, search_word, window_size=1):\n",
    "    \"\"\"\n",
    "    Find sentences containing the search word and include surrounding context.\n",
    "    Args:\n",
    "    - sentences: List of all sentences from the text.\n",
    "    - search_word: Word to search for.\n",
    "    - window_size: How many surrounding sentences to include before and after the target sentence.\n",
    "    Returns:\n",
    "    - List of sentences containing the word and their surrounding context.\n",
    "    \"\"\"\n",
    "    context_sentences = []\n",
    "    \n",
    "    # Iterate through sentences to find the ones containing the search word\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if re.search(rf'\\b{re.escape(search_word)}\\b', sentence, re.I):\n",
    "            # Capture surrounding context within the specified window size\n",
    "            start_idx = max(0, i - window_size)\n",
    "            end_idx = min(len(sentences), i + window_size + 1)\n",
    "            context = sentences[start_idx:end_idx]\n",
    "            context_sentences.extend(context)\n",
    "    \n",
    "    # Return the sentences containing the word and context around them\n",
    "    return context_sentences\n",
    "\n",
    "def truncate_sentence(sentence, max_words=20):\n",
    "    \"\"\"Limit sentence length to a maximum number of words.\"\"\"\n",
    "    words = word_tokenize(sentence)  # Tokenize the sentence into words\n",
    "    if len(words) > max_words:\n",
    "        truncated_sentence = ' '.join(words[:max_words]) + '...'  # Truncate and add ellipsis\n",
    "        return truncated_sentence\n",
    "    return sentence  # If sentence is short enough, return as-is\n",
    "\n",
    "def generate_unique_questions(sentences, num_questions=10, max_words=20):\n",
    "    \"\"\"\n",
    "    Generate unique questions from a list of sentences.\n",
    "    \"\"\"\n",
    "    question_starters = [\n",
    "        \"What is the significance of\",\n",
    "        \"Who is involved in\",\n",
    "        \"When did\",\n",
    "        \"Why does\",\n",
    "        \"How does\",\n",
    "        \"Where did\",\n",
    "        \"What are the details of\",\n",
    "        \"Explain the role of\",\n",
    "        \"Describe the impact of\",\n",
    "        \"What can you tell me about\"\n",
    "    ]\n",
    "\n",
    "    questions = set()  # Use a set to avoid duplicate questions\n",
    "    \n",
    "    # Repeat over sentences to ensure we get enough unique questions\n",
    "    while len(questions) < num_questions:\n",
    "        for sentence in sentences:\n",
    "            if len(questions) >= num_questions:\n",
    "                break\n",
    "            \n",
    "            # Randomly choose a question starter\n",
    "            starter = random.choice(question_starters)\n",
    "            \n",
    "            # Truncate the sentence to ensure it's not too long\n",
    "            truncated_sentence = truncate_sentence(sentence, max_words=max_words)\n",
    "            \n",
    "            # Formulate a question and ensure it ends with a \"?\"\n",
    "            question = f\"{starter} {truncated_sentence.strip()}?\"\n",
    "            questions.add(question)  # Add question to the set to ensure uniqueness\n",
    "    \n",
    "    # Return unique questions as a list\n",
    "    return list(questions)\n",
    "\n",
    "def generate_questions_v2(tokenized_text, search_word=None, num_questions=10, max_words=20, context_window=1):\n",
    "    # Split the text into sentences using nltk's sentence tokenizer\n",
    "    sentences = sent_tokenize(tokenized_text)\n",
    "    \n",
    "    # Get the sentences that contain the search word and their context\n",
    "    if search_word:\n",
    "        sentences = find_context_sentences(sentences, search_word, window_size=context_window)\n",
    "    \n",
    "    # Ensure that we generate unique questions\n",
    "    unique_questions = generate_unique_questions(sentences, num_questions=num_questions, max_words=max_words)\n",
    "    \n",
    "    return unique_questions\n",
    "\n",
    "\n",
    "# Check if the book exists in the DataFrame\n",
    "if book_title in books_df['Title'].values:\n",
    "    # Retrieve tokenized text for the book\n",
    "    tokenized_text = books_df.loc[books_df['Title'] == book_title, 'Tokenized Text'].values\n",
    "\n",
    "    # Check if tokenized_text is not empty\n",
    "    if len(tokenized_text) > 0:\n",
    "        tokenized_text = tokenized_text[0]  # Access the text\n",
    "        \n",
    "        # Enter the word you want to search for in the text\n",
    "        search_word = input(\"Enter a word to find related questions: \")\n",
    "        \n",
    "        # Generate questions from the tokenized text, with questions limited to 2-3 lines\n",
    "        generated_questions = generate_questions_v2(tokenized_text, search_word=search_word, num_questions=5, max_words=10, context_window=1)\n",
    "        \n",
    "        # Print the generated questions\n",
    "        print(\"\\nGenerated Questions:\")\n",
    "        for i, question in enumerate(generated_questions, 1):\n",
    "            print(f\"Q{i}: {question}\")\n",
    "    else:\n",
    "        print(f\"No tokenized text found for the book '{book_title}'.\")\n",
    "else:\n",
    "    print(f\"Book title '{book_title}' not found in the dataset.\")\n"
   ],
   "id": "5662c5dc86e4a333",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'book_title' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 90\u001B[0m\n\u001B[0;32m     86\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m unique_questions\n\u001B[0;32m     89\u001B[0m \u001B[38;5;66;03m# Check if the book exists in the DataFrame\u001B[39;00m\n\u001B[1;32m---> 90\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mbook_title\u001B[49m \u001B[38;5;129;01min\u001B[39;00m books_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTitle\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues:\n\u001B[0;32m     91\u001B[0m     \u001B[38;5;66;03m# Retrieve tokenized text for the book\u001B[39;00m\n\u001B[0;32m     92\u001B[0m     tokenized_text \u001B[38;5;241m=\u001B[39m books_df\u001B[38;5;241m.\u001B[39mloc[books_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTitle\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m book_title, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTokenized Text\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues\n\u001B[0;32m     94\u001B[0m     \u001B[38;5;66;03m# Check if tokenized_text is not empty\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'book_title' is not defined"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T23:53:30.524497Z",
     "start_time": "2024-10-23T23:53:30.519177Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7142c96aa5afbc4a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "59a5ee62c6f528e5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
